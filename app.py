import os
import io
import re
import json
import datetime as dt
from typing import List, Tuple, Dict, Any

import streamlit as st

# Gemini
try:
    import google.generativeai as genai
except Exception:
    genai = None

# PDF
try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None

# DOCX
try:
    import docx  # python-docx
except Exception:
    docx = None

# HTML
try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None

# Images
try:
    from PIL import Image
except Exception:
    Image = None

# Tables
try:
    import pandas as pd
except Exception:
    pd = None


APP_TITLE = "Voice Analyzer | Persona & Voice Spec (Gemini)"
MODEL_NAME = "gemini-3-pro-preview"


# -----------------------------
# Helpers
# -----------------------------
def now_str() -> str:
    return dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def clamp_text(text: str, max_chars: int) -> str:
    text = (text or "").strip()
    if max_chars <= 0:
        return ""
    if len(text) > max_chars:
        return text[:max_chars] + "\n\n[TRUNCATED]"
    return text


def bytes_of(uploaded_file) -> bytes:
    """Streamlit UploadedFile safe bytes getter (no cursor issues)."""
    try:
        return uploaded_file.getvalue()
    except Exception:
        # fallback
        uploaded_file.seek(0)
        return uploaded_file.read()


def extract_text_from_plain_bytes(raw: bytes, max_chars: int) -> str:
    try:
        txt = raw.decode("utf-8", errors="ignore")
    except Exception:
        txt = str(raw)
    return clamp_text(txt, max_chars)


def extract_text_from_pdf_bytes(raw: bytes, max_chars: int) -> str:
    if PdfReader is None:
        return ""
    try:
        reader = PdfReader(io.BytesIO(raw))
        chunks = []
        for page in reader.pages:
            t = page.extract_text() or ""
            if t.strip():
                chunks.append(t)
        return clamp_text("\n\n".join(chunks), max_chars)
    except Exception:
        return ""


def extract_text_from_docx_bytes(raw: bytes, max_chars: int) -> str:
    if docx is None:
        return ""
    try:
        d = docx.Document(io.BytesIO(raw))
        paras = [p.text for p in d.paragraphs if p.text and p.text.strip()]
        return clamp_text("\n".join(paras), max_chars)
    except Exception:
        return ""


def extract_text_from_html_bytes(raw: bytes, max_chars: int) -> str:
    try:
        html = raw.decode("utf-8", errors="ignore")
    except Exception:
        return ""

    if BeautifulSoup is None:
        # best-effort strip tags
        text = re.sub(r"<[^>]+>", " ", html)
        text = re.sub(r"\s+", " ", text)
        return clamp_text(text, max_chars)

    try:
        soup = BeautifulSoup(html, "html.parser")
        text = soup.get_text(separator="\n")
        text = re.sub(r"\n{3,}", "\n\n", text)
        return clamp_text(text, max_chars)
    except Exception:
        return ""


def extract_text_from_rtf_bytes(raw: bytes, max_chars: int) -> str:
    # Minimal RTF stripper (best-effort)
    try:
        rtf = raw.decode("utf-8", errors="ignore")
    except Exception:
        return ""
    text = re.sub(r"{\\.*?}|\\[a-zA-Z]+\d* ?", " ", rtf)
    text = re.sub(r"[{}]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return clamp_text(text, max_chars)


def load_image_from_bytes(raw: bytes):
    if Image is None:
        return None
    try:
        return Image.open(io.BytesIO(raw)).convert("RGB")
    except Exception:
        return None


def summarize_dataframe_voice(df, sheet_name: str, max_rows_per_col: int, max_cell_chars: int) -> str:
    """Turn a table into a 'voice evidence' summary text."""
    if df is None or df.empty:
        return ""

    blocks = [f"=== [TABLE SHEET: {sheet_name}] ==="]
    for col in df.columns:
        try:
            series = df[col].dropna().astype(str).str.strip()
        except Exception:
            continue

        # filter blanks after strip
        series = series[series != ""]
        if series.empty:
            continue

        samples = series.head(max_rows_per_col).tolist()
        if not samples:
            continue

        blocks.append(f"ã€æ¬„ä½ï¼š{str(col)}ã€‘")
        for s in samples:
            s = s[:max_cell_chars]
            blocks.append(f"- {s}")

    blocks.append(f"=== [/TABLE SHEET: {sheet_name}] ===")
    return "\n".join(blocks)


def extract_text_from_table_bytes(raw: bytes, filename: str, max_chars: int, max_rows_per_col: int = 8) -> str:
    """
    CSV / Excel -> voice evidence text
    - Excel: summarize up to first few sheets (to avoid huge prompts)
    """
    if pd is None:
        return ""

    name = filename.lower()
    bio = io.BytesIO(raw)

    try:
        if name.endswith(".csv"):
            # CSV: let pandas infer encoding best-effort
            df = pd.read_csv(bio)
            text = summarize_dataframe_voice(df, "CSV", max_rows_per_col=max_rows_per_col, max_cell_chars=160)
            return clamp_text(text, max_chars)

        # Excel: read multiple sheets
        sheets: Dict[str, Any] = pd.read_excel(bio, sheet_name=None)
        blocks = [f"=== [EXCEL FILE: {filename}] ==="]

        # limit sheets to avoid explosion
        for i, (sname, df) in enumerate(sheets.items()):
            if i >= 5:
                blocks.append("...ï¼ˆå…¶é¤˜å·¥ä½œè¡¨ç•¥éï¼‰")
                break
            blocks.append(summarize_dataframe_voice(df, sname, max_rows_per_col=max_rows_per_col, max_cell_chars=160))

        blocks.append(f"=== [/EXCEL FILE: {filename}] ===")
        return clamp_text("\n\n".join([b for b in blocks if b.strip()]), max_chars)

    except Exception:
        return ""


def build_prompt(
    sample_blocks: List[str],
    notes: str,
    constraints: str,
    output_language: str,
    attachments_report: str,
) -> str:
    samples_joined = "\n\n".join([b for b in sample_blocks if b.strip()]).strip()
    notes = (notes or "").strip()
    constraints = (constraints or "").strip()

    lang_rule = {
        "ç¹é«”ä¸­æ–‡": "è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚",
        "English": "Please write in English.",
        "æ—¥æœ¬èª": "æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚",
    }.get(output_language, "è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚")

    return f"""
ä½ æ˜¯ä¸€å€‹ã€Œèªæ„Ÿ/é¢¨æ ¼/åƒ¹å€¼è§€ã€åˆ†æå™¨ã€‚ä½ çš„ä»»å‹™ï¼šå¾æ¨£æœ¬æ–‡æœ¬ï¼ˆä»¥åŠå¿…è¦æ™‚å°åœ–ç‰‡å…§å®¹çš„ç†è§£ï¼‰æ­¸ç´ä½œè€…çš„ Persona èˆ‡å¯åŸ·è¡Œçš„å¯«ä½œè¦æ ¼ï¼ˆVoice Specï¼‰ï¼Œä¸¦è¼¸å‡ºå¯ç›´æ¥è²¼å…¥å¦ä¸€å€‹å°ˆæ¡ˆå°åŒ…çš„ VOICE CONTEXTã€‚

ã€é‡è¦è¦å‰‡ã€‘
1) ã€Œå¯æŠ½å–æ–‡æœ¬çš„æª”æ¡ˆã€= è­‰æ“šå±¤ï¼šä½ åªèƒ½å¾é€™è£¡æ­¸ç´èªæ„Ÿç‰¹å¾µã€å¥æ³•ã€ç«‹å ´ã€ç¯€å¥ï¼Œä¸å¯æ†‘ç©ºæé€ ä½œè€…èƒŒæ™¯ã€‚
2) ã€ŒExcel/CSVã€= è­‰æ“šå±¤ï¼šæˆ‘å·²å°‡è¡¨æ ¼è½‰æˆã€Œæ¬„ä½ï¼‹æ¨£ä¾‹ã€çš„èªæ„Ÿæ‘˜è¦ï¼Œä½ å¯ç”¨ä¾†åˆ¤æ–·å‘½åé¢¨æ ¼ã€å¥å‹ç¿’æ…£ã€èªå½™åå¥½ã€‚
3) ã€Œåœ–ç‰‡æª”ã€= è­‰æ“šå±¤ï¼šä½ å¯ä»¥æ ¹æ“šåœ–ç‰‡ä¸­å¯è¦‹æ–‡å­—/ç‰ˆé¢/èªæ°£ç”¨æ³•åšæ­¸ç´ï¼ˆä¸è¦è‡†æ¸¬çœ‹ä¸è¦‹çš„å…§å®¹ï¼‰ã€‚
4) ã€Œæˆ‘çš„ç­†è¨˜ã€= èªå¢ƒæ ¡æº–å±¤ï¼šå¯ä»¥è£œè¶³ä½œè€…å®šä½/åƒ¹å€¼è§€è„ˆçµ¡ï¼›è‹¥èˆ‡è­‰æ“šå±¤è¡çªï¼Œå¿…é ˆæŒ‡å‡ºè¡çªä¸¦çµ¦å‡ºå…©å¥—ç‰ˆæœ¬ï¼ˆV-A: ä»¥è­‰æ“šç‚ºæº–ï¼›V-B: ä»¥ç­†è¨˜ç‚ºæº–ï¼‰ã€‚
5) ç¦æ­¢å¼•ç”¨ä»»ä½•æ¨£æœ¬æ–‡æœ¬åŸå¥è¶…é 25 å­—ï¼›ä¸å¾—å¤§é‡æŠ„éŒ„ã€‚
6) ä½ çš„è¼¸å‡ºå¿…é ˆå¯åŸ·è¡Œï¼šè¦èƒ½è®“å¦ä¸€å€‹ AI æŒ‰è¦æ ¼ç©©å®šæ¨¡ä»¿å¯«ä½œã€‚
7) {lang_rule}

ã€ä½ æ”¶åˆ°çš„é™„ä»¶ç‹€æ…‹ï¼ˆä¾›ä½ åˆ¤æ–·è­‰æ“šå¼·åº¦ï¼‰ã€‘
{attachments_report}

ã€è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼ï¼‰ã€‘
A) Persona Briefï¼ˆå¯è®€ï¼‰
- ä½œè€…ä¸–ç•Œè§€ä¸€å¥è©±ï¼š
- å°è®€è€…çš„å®šä½ï¼ˆä¸Šå°ä¸‹/ä¸¦è‚©/æŒ‘é‡/å°è©±ï¼‰ï¼š
- æ ¸å¿ƒä¿¡å¿µ/åƒ¹å€¼è§€ï¼ˆ3â€“7æ¢ï¼Œå¥å‹åŒ–ï¼‰ï¼š
- å‹•æ©Ÿé‚Šç•Œï¼ˆä»–ç‚ºä»€éº¼å¯«ã€ä»–ä¸åšä»€éº¼ï¼‰ï¼š
- å…è¨±çš„æ¨¡ç³Šèˆ‡ç•™ç™½ï¼ˆå“ªäº›å¯ä»¥ä¸è¬›æ­»ï¼‰ï¼š
- ç¦èª/ç¦å¥—è·¯ï¼ˆå«ç†ç”±ï¼‰ï¼š

B) Voice Specï¼ˆå¯åŸ·è¡Œï¼‰
- tone_mixï¼ˆ%ï¼‰ï¼šå†·éœ__ / çŠ€åˆ©__ / å¹½é»˜__ / æº«åº¦__
- sentence_rhythmï¼šçŸ­å¥æ¯”ä¾‹__%ï¼›æ¯æ®µ__â€“__è¡Œï¼›è½‰æŠ˜é »ç‡__
- stance_rulesï¼šå¦‚ä½•ä¸‹çµè«–/å¦‚ä½•ç•™ç™½/å¦‚ä½•åå•
- lexical_rulesï¼šå¸¸ç”¨è©/é¿å…è©/ç¦è©
- structure_rulesï¼šå¸¸ç”¨æ¨ç†é †åºï¼ˆä¾‹ï¼šç¾è±¡â†’å°ç…§â†’æ¨è«–â†’é¸é …ï¼‰
- do_notï¼šçµ•å°ç¦æ­¢äº‹é …
- sample_linesï¼ˆ<=5å¥ï¼Œæ¯å¥<=25å­—ï¼Œæ¨¡ä»¿ç”¨ã€ä¸å¯å¼•ç”¨åŸæ–‡ï¼‰ï¼š

C) å¯ç›´æ¥è²¼å…¥å°åŒ…çš„ VOICE CONTEXTï¼ˆè«‹ç”¨ä¸€å€‹ Markdown code block åŒ…èµ·ä¾†ï¼‰
å…§å®¹éœ€é•·å¾—åƒé€™æ¨£ï¼ˆä½ è¦å¡«æ»¿æ¬„ä½ï¼‰ï¼š
=== [VOICE CONTEXT | EDITABLE] ===
[PERSONA LOG]
...
[VOICE SPEC]
...
=== [/VOICE CONTEXT] ===

D) å¿«é€Ÿé©—æ”¶æ¸…å–®ï¼ˆè®“ç¸½ç·¨è¼¯å¿«é€Ÿæª¢æŸ¥æ˜¯å¦åƒä»–ï¼‰
- 3 å€‹ã€Œåƒã€çš„åˆ¤æº–
- 3 å€‹ã€Œä¸åƒã€çš„è­¦æˆ’

ã€é¡å¤–é™åˆ¶/åå¥½ï¼ˆè‹¥æœ‰ï¼Œå¿…é ˆéµå®ˆï¼‰ã€‘
{constraints if constraints else "ï¼ˆç„¡ï¼‰"}

ã€æ¨£æœ¬æ–‡æœ¬ï¼ˆè­‰æ“šå±¤ï¼‰ã€‘
{samples_joined if samples_joined else "ï¼ˆç›®å‰æ²’æœ‰å¯æŠ½å–æ–‡æœ¬ã€‚è«‹å…ˆæŒ‡å‡ºä¸è¶³ï¼Œä¸¦åœ¨å¯æ¨è«–ç¯„åœå…§çµ¦ä¸€ç‰ˆã€ä½ä¿¡å¿ƒã€è¦æ ¼ï¼Œæé†’éœ€è¦æ›´å¤šæ¨£æœ¬æˆ–è«‹æˆ‘è²¼é—œéµæ®µè½ã€‚ï¼‰"}

ã€æˆ‘çš„ç­†è¨˜ï¼ˆèªå¢ƒæ ¡æº–å±¤ï¼‰ã€‘
{notes if notes else "ï¼ˆæœªæä¾›ï¼‰"}
""".strip()


def call_gemini_multimodal(
    api_key: str,
    prompt_text: str,
    images: List,
    temperature: float,
    max_output_tokens: int,
) -> str:
    if genai is None:
        raise RuntimeError("google-generativeai æœªå®‰è£æˆ–åŒ¯å…¥å¤±æ•—ã€‚è«‹ç¢ºèª requirements.txtã€‚")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(MODEL_NAME)

    parts = [prompt_text]
    for img in images:
        if img is not None:
            parts.append(img)

    resp = model.generate_content(
        parts,
        generation_config={
            "temperature": temperature,
            "max_output_tokens": max_output_tokens,
        },
    )

    text = getattr(resp, "text", None)
    if text:
        return text

    # fallback for SDK variations
    try:
        return resp.candidates[0].content.parts[0].text
    except Exception:
        return str(resp)


# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ§¬", layout="wide")
st.title("ğŸ§¬ Voice Analyzerï¼ˆå¤šæ ¼å¼ + Excel/CSV + åœ–æª”ï¼‰")
st.caption(f"å›ºå®šæ¨¡å‹ï¼š{MODEL_NAME}ï½œè¼¸å…¥ï¼šæ–‡ä»¶/è¡¨æ ¼/æˆªåœ– + ä½ çš„ç­†è¨˜ï½œè¼¸å‡ºï¼šPersona Brief + Voice Spec + VOICE CONTEXTï¼ˆå¯è²¼å…¥å°åŒ…ï¼‰")

with st.sidebar:
    st.subheader("ğŸ”‘ Gemini API Key")
    api_key = st.text_input("GEMINI_API_KEY", type="password", value=os.getenv("GEMINI_API_KEY", ""))

    st.divider()
    st.subheader("âš™ï¸ ç”Ÿæˆåƒæ•¸")
    temperature = st.slider("temperature", 0.0, 1.0, 0.4, 0.05)
    max_output_tokens = st.slider("max_output_tokens", 512, 8192, 4096, 256)

    st.divider()
    st.subheader("ğŸ§  è­‰æ“šé™åˆ¶ï¼ˆé¿å… prompt çˆ†ç‚¸ï¼‰")
    max_chars_per_file = st.slider("æ¯å€‹å¯æŠ½æ–‡æœ¬æª”çš„æœ€å¤§å­—å…ƒæ•¸", 5000, 200000, 60000, 5000)
    max_total_chars = st.slider("å…¨éƒ¨æ–‡æœ¬åˆè¨ˆæœ€å¤§å­—å…ƒæ•¸", 20000, 500000, 200000, 10000)
    max_rows_per_col = st.slider("Excel/CSV æ¯æ¬„æœ€å¤šå–æ¨£ç­†æ•¸", 3, 20, 8, 1)

    st.divider()
    output_language = st.selectbox("è¼¸å‡ºèªè¨€", ["ç¹é«”ä¸­æ–‡", "English", "æ—¥æœ¬èª"], index=0)

    st.divider()
    save_history = st.toggle("ä¿å­˜åˆ°æœ¬æ©Ÿ Session History", value=True)


col1, col2 = st.columns([1, 1], gap="large")

with col1:
    st.subheader("1) ä¸Šå‚³æ¨£æœ¬ï¼ˆæ”¯æ´å¤šæ ¼å¼ï¼‰")
    st.write(
        "æ”¯æ´ï¼štxt / md / pdf / docx / html / rtf / csv / xlsx / xls "
        "+ png/jpg/jpeg/webpï¼ˆåœ–æª”èµ°å¤šæ¨¡æ…‹ï¼›Excel/CSV æœƒè½‰æˆæ¬„ä½ï¼‹æ¨£ä¾‹çš„èªæ„Ÿæ‘˜è¦ï¼‰"
    )

    uploads = st.file_uploader(
        "ä¸Šå‚³æª”æ¡ˆï¼ˆå¯å¤šæª”ï¼‰",
        type=["txt", "md", "pdf", "docx", "html", "htm", "rtf", "csv", "xlsx", "xls", "png", "jpg", "jpeg", "webp"],
        accept_multiple_files=True
    )

    pasted = st.text_area(
        "æˆ–ç›´æ¥è²¼æ¨£æœ¬æ–‡æœ¬ï¼ˆå¯èˆ‡ä¸Šå‚³åŒæ™‚ç”¨ï¼‰",
        height=220,
        placeholder="è²¼ä¸Š 1â€“3 ç¯‡ä»£è¡¨æ€§æ–‡å­—â€¦"
    )

    st.subheader("2) ä½ çš„ç­†è¨˜ï¼ˆèªå¢ƒæ ¡æº–å±¤ï¼‰")
    notes = st.text_area(
        "ä½ å°é€™å€‹äººçš„ç†è§£ï¼šåƒ¹å€¼è§€ã€ç¦å¿Œã€è®€è€…é—œä¿‚ã€å¯«ä½œç›®çš„ã€ä¸å¯ç¢°è§¸çš„é»â€¦",
        height=220,
        placeholder="ä¾‹ï¼šä»–è¨å­é›æ¹¯ï¼›å¯«ä½œè¦æ¨å‹•ç”¢æ¥­æ”¹é©ï¼›å°è®€è€…æ˜¯ä¸¦è‚©è¨è«–è€Œéæ•™å­¸â€¦"
    )

with col2:
    st.subheader("3) é¡å¤–é™åˆ¶ / åå¥½ï¼ˆå¯é¸ï¼‰")
    constraints = st.text_area(
        "ä¾‹å¦‚ï¼šä¸è¦æƒ…ç·’å‹’ç´¢ã€ä¸è¦æ•™æ¢å¼çµè«–ã€è¦å¤šåå•ã€é¿å…ã€ä½ æ‡‰è©²ã€å¥å‹â€¦",
        height=180,
        placeholder="å¯ç•™ç©º"
    )

    st.subheader("4) ä¸€éµç”¢å‡º")
    run = st.button("ğŸš€ é–‹å§‹èªæ„Ÿåˆ†æ", type="primary", use_container_width=True)

    st.info(
        "ç­–ç•¥ï¼šèƒ½æŠ½å­—å°±æŠ½å­—ï¼›Excel/CSV è½‰æˆã€æ¬„ä½ï¼‹æ¨£ä¾‹ã€æ‘˜è¦ï¼›åœ–æª”ç›´æ¥é™„çµ¦æ¨¡å‹ã€‚\n"
        "è‹¥æƒæ PDF æŠ½ä¸åˆ°å­—ï¼Œå»ºè­°æ”¹å‚³å¯é¸å–æ–‡å­—çš„ PDFï¼Œæˆ–ç›´æ¥ä¸Šå‚³æˆªåœ–/åœ–ç‰‡ã€‚"
    )


# -----------------------------
# Prepare evidence
# -----------------------------
sample_blocks: List[str] = []
images: List = []
report_lines: List[str] = []
total_chars = 0

if uploads:
    for f in uploads:
        filename = f.name
        ext = filename.lower().split(".")[-1] if "." in filename else ""
        mime = getattr(f, "type", "")

        raw = bytes_of(f)

        # Image files -> multimodal
        if ext in ("png", "jpg", "jpeg", "webp"):
            img = load_image_from_bytes(raw)
            if img is not None:
                images.append(img)
                report_lines.append(f"- âœ… åœ–æª”ï¼š{filename}ï¼ˆå¤šæ¨¡æ…‹å·²é™„åŠ ï¼‰")
            else:
                report_lines.append(f"- âš ï¸ åœ–æª”ï¼š{filename}ï¼ˆè®€å–å¤±æ•—ï¼Œè«‹æ›æ ¼å¼æˆ–é‡å‚³ï¼‰")
            continue

        # Text extractable
        extracted = ""

        if ext in ("csv", "xlsx", "xls"):
            extracted = extract_text_from_table_bytes(
                raw=raw,
                filename=filename,
                max_chars=max_chars_per_file,
                max_rows_per_col=max_rows_per_col
            )

        elif ext in ("txt", "md"):
            extracted = extract_text_from_plain_bytes(raw, max_chars=max_chars_per_file)

        elif ext in ("pdf",):
            extracted = extract_text_from_pdf_bytes(raw, max_chars=max_chars_per_file)

        elif ext in ("docx",):
            extracted = extract_text_from_docx_bytes(raw, max_chars=max_chars_per_file)

        elif ext in ("html", "htm"):
            extracted = extract_text_from_html_bytes(raw, max_chars=max_chars_per_file)

        elif ext in ("rtf",):
            extracted = extract_text_from_rtf_bytes(raw, max_chars=max_chars_per_file)

        else:
            # fallback: try read as text
            extracted = extract_text_from_plain_bytes(raw, max_chars=max_chars_per_file)

        extracted = (extracted or "").strip()
        if extracted:
            remaining = max_total_chars - total_chars
            if remaining <= 0:
                report_lines.append(f"- â­ï¸ {filename}ï¼ˆå¯æŠ½å­—ä½†å·²é”ç¸½å­—å…ƒä¸Šé™ï¼Œç•¥éï¼‰")
                continue

            if len(extracted) > remaining:
                extracted = extracted[:remaining] + "\n\n[TRUNCATED_BY_TOTAL_LIMIT]"

            total_chars += len(extracted)
            sample_blocks.append(f"=== [FILE: {filename} | {mime}] ===\n{extracted}\n=== [/FILE] ===")
            report_lines.append(f"- âœ… å¯æŠ½å­—ï¼š{filename}ï¼ˆç´å…¥ {len(extracted):,} å­—ï¼‰")
        else:
            report_lines.append(
                f"- âš ï¸ {filename}ï¼ˆæŠ½ä¸åˆ°æ–‡å­—/æ‘˜è¦ï¼›å¯èƒ½æ˜¯æƒæPDFæˆ–æ ¼å¼ä¸æ”¯æ´ã€‚å»ºè­°è²¼é—œéµæ®µè½æˆ–æ”¹å‚³å¯é¸å–æ–‡å­—ç‰ˆæœ¬ï¼‰"
            )

if pasted.strip():
    remaining = max_total_chars - total_chars
    paste_txt = clamp_text(pasted.strip(), remaining)
    if paste_txt:
        sample_blocks.append(f"=== [PASTED] ===\n{paste_txt}\n=== [/PASTED] ===")
        report_lines.append(f"- âœ… ç›´æ¥è²¼ä¸Šæ–‡æœ¬ï¼ˆç´å…¥ {len(paste_txt):,} å­—ï¼‰")
        total_chars += len(paste_txt)
    else:
        report_lines.append("- â­ï¸ ç›´æ¥è²¼ä¸Šæ–‡æœ¬ï¼ˆå·²é”ç¸½å­—å…ƒä¸Šé™ï¼Œç•¥éï¼‰")

attachments_report = "\n".join(report_lines) if report_lines else "- ï¼ˆæœªä¸Šå‚³ä»»ä½•é™„ä»¶ï¼‰"

prompt_text = build_prompt(
    sample_blocks=sample_blocks,
    notes=notes,
    constraints=constraints,
    output_language=output_language,
    attachments_report=attachments_report
)

# history
if "history" not in st.session_state:
    st.session_state.history = []

if run:
    if not api_key.strip():
        st.error("ç¼ºå°‘ GEMINI_API_KEYã€‚è«‹åœ¨å´æ¬„è²¼ä¸Šã€‚")
    else:
        with st.expander("ğŸ“ é™„ä»¶ç‹€æ…‹ï¼ˆæœƒå½±éŸ¿è­‰æ“šå¼·åº¦ï¼‰", expanded=True):
            st.write(attachments_report)
            st.caption(f"æ–‡æœ¬åˆè¨ˆç´å…¥ï¼š{total_chars:,} å­—ï½œåœ–ç‰‡é™„åŠ ï¼š{len(images)} å¼µ")

        with st.spinner("ç”Ÿæˆä¸­â€¦"):
            try:
                output = call_gemini_multimodal(
                    api_key=api_key.strip(),
                    prompt_text=prompt_text,
                    images=images,
                    temperature=temperature,
                    max_output_tokens=max_output_tokens,
                )
            except Exception as e:
                st.error(f"å‘¼å« Gemini å¤±æ•—ï¼š{e}")
                output = ""

        if output:
            st.subheader("âœ… ç”Ÿæˆçµæœ")
            st.write(output)

            if save_history:
                st.session_state.history.insert(
                    0,
                    {
                        "ts": now_str(),
                        "model": MODEL_NAME,
                        "temperature": temperature,
                        "max_output_tokens": max_output_tokens,
                        "output_language": output_language,
                        "constraints": constraints,
                        "total_text_chars": total_chars,
                        "images_count": len(images),
                        "output": output,
                    }
                )

            st.divider()
            st.subheader("â¬‡ï¸ åŒ¯å‡º")
            export_payload = {
                "meta": {
                    "ts": now_str(),
                    "model": MODEL_NAME,
                    "temperature": temperature,
                    "max_output_tokens": max_output_tokens,
                    "output_language": output_language,
                },
                "evidence": {
                    "attachments_report": attachments_report,
                    "total_text_chars": total_chars,
                    "images_count": len(images),
                },
                "input": {
                    "constraints": constraints,
                    "notes": notes,
                },
                "output": output,
            }

            st.download_button(
                "ä¸‹è¼‰ JSONï¼ˆå«è¼¸å…¥/è­‰æ“šæ‘˜è¦/è¼¸å‡ºï¼‰",
                data=json.dumps(export_payload, ensure_ascii=False, indent=2),
                file_name=f"voice_analysis_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )
            st.download_button(
                "ä¸‹è¼‰ TXTï¼ˆåªå«è¼¸å‡ºï¼‰",
                data=output,
                file_name=f"voice_analysis_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain; charset=utf-8",
                use_container_width=True
            )

st.divider()
st.subheader("ğŸ“š Session Historyï¼ˆæœ¬æ¬¡ç€è¦½å™¨æœŸé–“ï¼‰")
if st.session_state.history:
    for i, item in enumerate(st.session_state.history[:10], start=1):
        with st.expander(
            f"{i}. {item['ts']} | {item['model']} | temp={item['temperature']} | img={item['images_count']} | chars={item['total_text_chars']:,}"
        ):
            st.write(item["output"])
else:
    st.caption("å°šç„¡ç´€éŒ„ã€‚")
